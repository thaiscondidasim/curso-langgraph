# Li√ß√£o 2 - Breakpoints

### üß† **Contexto**

O **streaming** permite que o LangGraph emita o estado do grafo a cada passo da execu√ß√£o. Isso cria a base para implementar o conceito de **Human-in-the-Loop** ‚Äî ou seja, incluir um ser humano no controle de partes cr√≠ticas do fluxo.

Para `human-in-the-loop`, frequentemente queremos ver as sa√≠das do nosso gr√°fico enquanto ele est√° em execu√ß√£o.

Estabelecemos as bases para isso com streaming.

## Objetivos

Agora, vamos falar sobre as motiva√ß√µes para `human-in-the-loop`:

(1) `Aprova√ß√£o` - Podemos interromper nosso agente, exibir o estado para um usu√°rio e permitir que ele aceite uma a√ß√£o.

(2) `Depura√ß√£o` - Podemos retroceder o gr√°fico para reproduzir ou evitar problemas.

(3) `Edi√ß√£o` - Voc√™ pode modificar o estado.

O LangGraph oferece v√°rias maneiras de obter ou atualizar o estado do agente para oferecer suporte a v√°rios fluxos de trabalho `human-in-the-loop`.

Primeiro, apresentaremos [pontos de interrup√ß√£o](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/#simple-usage), que fornecem uma maneira simples de interromper o gr√°fico em etapas espec√≠ficas.

Mostraremos como isso permite a `aprova√ß√£o` do usu√°rio.

---

### ‚úÖ Tr√™s principais **casos de uso** para Human-in-the-Loop:

1. **Aprova√ß√£o de a√ß√µes sens√≠veis**
    
    Exemplo: se o agente vai usar uma *ferramenta* que escreve em um banco de dados ou sistema externo, o humano pode precisar aprovar antes.
    
2. **Depura√ß√£o e reexecu√ß√£o (debugging)**
    
    Permite **pausar o grafo**, inspecionar o estado e **reexecutar** a partir de qualquer ponto, √∫til para diagnosticar e evitar erros.
    
3. **Edi√ß√£o direta do estado**
    
    O humano pode modificar ou adicionar informa√ß√µes no estado do agente antes de continuar a execu√ß√£o.
    

---

### üõë **Breakpoints: interrompendo a execu√ß√£o do grafo**

Vamos reconsiderar o agente simples com o qual trabalhamos no M√≥dulo 1.

Vamos supor que estamos preocupados com o uso da ferramenta: queremos aprovar o agente para usar qualquer uma de suas ferramentas.

Tudo o que precisamos fazer √© simplesmente compilar o grafo com `interrupt_before=["tools"]`, onde `tools` √© o nosso n√≥ de ferramentas.

Isso significa que a execu√ß√£o ser√° interrompida antes do n√≥ `tools`, que executa a chamada da ferramenta.

LangGraph oferece **breakpoints** que interrompem a execu√ß√£o do grafo em pontos espec√≠ficos. Isso √© feito com:

- `interrupt_before`: pausa antes de um n√≥ espec√≠fico
- `interrupt_after`: pausa ap√≥s um n√≥

---

### üß™ Exemplo com ferramenta (tool node)

1. Suponha que temos um **n√≥ de ferramenta**, que executa uma a√ß√£o automatizada.
2. Queremos aprovar qualquer a√ß√£o feita com ferramentas.
3. Definimos o grafo com `interrupt_before="tools"`.

üìç Agora, o grafo:

- Roda at√© o n√≥ da ferramenta;
- **Pausa a execu√ß√£o**;
- Permite que o humano aprove ou rejeite a continua√ß√£o.

```python
from langchain_openai import ChatOpenAI

def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

# This will be a tool
def add(a: int, b: int) -> int:
    """Adds a and b.

    Args:
        a: first int
        b: second int
    """
    return a + b

def divide(a: int, b: int) -> float:
    """Divide a by b.

    Args:
        a: first int
        b: second int
    """
    return a / b

tools = [add, multiply, divide]
llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools(tools)
```

```python
from IPython.display import Image, display

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition, ToolNode

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

# System message
sys_msg = SystemMessage(content="You are a helpful assistant tasked with performing arithmetic on a set of inputs.")

# Node
def assistant(state: MessagesState):
   return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

# Graph
builder = StateGraph(MessagesState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine the control flow
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "assistant")

memory = MemorySaver()
graph = builder.compile(interrupt_before=["tools"], checkpointer=memory)

# Show
display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```

![image.png](attachment:fd692428-4433-4c53-b844-181269b983b6:image.png)

```python
# Input
initial_input = {"messages": HumanMessage(content="Multiply 2 and 3")}

# Thread
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

```python
================================[1m Human Message [0m=================================

Multiply 2 and 3
==================================[1m Ai Message [0m==================================
Tool Calls:
  multiply (call_oFkGpnO8CuwW9A1rk49nqBpY)
 Call ID: call_oFkGpnO8CuwW9A1rk49nqBpY
  Args:
    a: 2
    b: 3
```

Podemos obter o estado e observar o pr√≥ximo n√≥ a ser chamado.

Esta √© uma boa maneira de ver que o gr√°fico foi interrompido.

```python
state = graph.get_state(thread)
state.next
```

---

### üëÅÔ∏è Visualizando o estado parado

Usamos `graph.get_state(thread_id)` para obter o **checkpoint atual**.

Podemos tamb√©m ver:

- Qual √© o pr√≥ximo n√≥ (`next_node`)
- O estado acumulado da conversa
- O hist√≥rico completo com `graph.get_state_history()`

---

### ‚ñ∂Ô∏è **Continuando a partir do breakpoint**

Agora, vamos apresentar um truque interessante.

Quando invocamos o grafo com `None`, ele simplesmente continua a partir do √∫ltimo ponto de verifica√ß√£o de estado!

![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbae7985b747dfed67775d_breakpoints1.png)

Para maior clareza, o LangGraph reemitir√° o estado atual, que cont√©m a `AIMessage` com a chamada da ferramenta.

E ent√£o ele prosseguir√° com a execu√ß√£o dos seguintes passos no grafo, que come√ßam com o n√≥ da ferramenta.

Vemos que o n√≥ da ferramenta √© executado com essa chamada da ferramenta e √© passado de volta ao modelo de bate-papo para nossa resposta final.

Depois que o humano aprova, usamos:

```python
graph.stream(None, thread_id=...)

```

Esse comando:

- **Reemite o estado atual**
- **Continua a execu√ß√£o a partir do ponto de parada**

```python
for event in graph.stream(None, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

```python
==================================[1m Ai Message [0m==================================
Tool Calls:
  multiply (call_oFkGpnO8CuwW9A1rk49nqBpY)
 Call ID: call_oFkGpnO8CuwW9A1rk49nqBpY
  Args:
    a: 2
    b: 3
=================================[1m Tool Message [0m=================================
Name: multiply

6
==================================[1m Ai Message [0m==================================

The result of multiplying 2 and 3 is 6.
```

Agora, vamos unir tudo isso com uma etapa espec√≠fica de aprova√ß√£o do usu√°rio que aceita a entrada do usu√°rio.

```python
# Input
initial_input = {"messages": HumanMessage(content="Multiply 2 and 3")}

# Thread
thread = {"configurable": {"thread_id": "2"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()

# Get user feedback
user_approval = input("Do you want to call the tool? (yes/no): ")

# Check approval
if user_approval.lower() == "yes":
    
    # If approved, continue the graph execution
    for event in graph.stream(None, thread, stream_mode="values"):
        event['messages'][-1].pretty_print()
        
else:
    print("Operation cancelled by user.")
```

```python
================================[1m Human Message [0m=================================

Multiply 2 and 3
==================================[1m Ai Message [0m==================================
Tool Calls:
  multiply (call_tpHvTmsHSjSpYnymzdx553SU)
 Call ID: call_tpHvTmsHSjSpYnymzdx553SU
  Args:
    a: 2
    b: 3
==================================[1m Ai Message [0m==================================
Tool Calls:
  multiply (call_tpHvTmsHSjSpYnymzdx553SU)
 Call ID: call_tpHvTmsHSjSpYnymzdx553SU
  Args:
    a: 2
    b: 3
=================================[1m Tool Message [0m=================================
Name: multiply

6
==================================[1m Ai Message [0m==================================

The result of multiplying 2 and 3 is 6.
```

---

### ‚úÖ Exemplo com input de aprova√ß√£o:

1. O humano envia `"yes"` como input
2. O sistema chama novamente o `stream(None)` com o mesmo `thread_id`
3. O grafo continua do ponto onde parou (ferramenta) e segue at√© o fim

---

### üåê Usando a **API do LangGraph**

Al√©m de definir o breakpoint no c√≥digo, a **API permite passar interrup√ß√µes dinamicamente**:

```python
client.stream(interrupt_before="tools", ...)

```

üí° Isso √© √∫til porque voc√™ n√£o precisa recompilar o grafo toda vez. Basta especificar o n√≥ onde quer parar.

---

### üì¶ No Studio (interface gr√°fica)

1. O grafo roda localmente no Studio
2. Podemos aplicar `interrupt_before` via c√≥digo (como antes) **ou**
3. **Via API**: basta passar o argumento no momento da chamada

---

### üß∞ Resumo t√©cnico

| Recurso | O que faz |
| --- | --- |
| `interrupt_before="node"` | Interrompe a execu√ß√£o **antes** do n√≥ especificado |
| `graph.get_state()` | Recupera o **checkpoint atual** do grafo |
| `graph.stream(None)` | **Continua a execu√ß√£o** a partir do ponto em que parou |
| API com `interrupt_before` | Permite aplicar breakpoints dinamicamente, sem alterar o c√≥digo do grafo |
| Studio | Interface visual com suporte a breakpoints e aprova√ß√£o manual |

---

### ‚úÖ Conclus√£o: por que isso √© √∫til?

- Torna o LangGraph ideal para **fluxos cr√≠ticos**, como automa√ß√£o com valida√ß√£o humana.
- Permite **debug interativo**.
- Cria **assistentes h√≠bridos**, com decis√µes automatizadas e interven√ß√µes humanas.

---

Se quiser, posso gerar um exemplo completo de **fluxo com breakpoint e aprova√ß√£o humana**, usando `LangGraph + API`, ou integrar isso com uma interface web. Deseja seguir com isso?