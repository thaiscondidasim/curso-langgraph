**ReAct** √© uma arquitetura de agente gen√©rica muito popular, que realiza tr√™s etapas principais:

1. **Agir (Act):**
    
    Permite que o agente chame ferramentas espec√≠ficas.
    
2. **Observar (Observe):**
    
    Passa a sa√≠da da ferramenta de volta para o modelo LLM.
    
3. **Raciocinar (Reason):**
    
    O modelo interpreta essa sa√≠da, decide o que fazer em seguida ‚Äî seja chamar outra ferramenta ou encerrar com uma resposta em linguagem natural.
    

Essa arquitetura √© bastante cl√°ssica e amplamente utilizada.

![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab7453080e6802cd1703_agent-memory1.png)

---

### üß† Agora vamos estender isso com um conceito simples: **mem√≥ria**

Assim como antes, definimos nossa chave da API e criamos um agente com tr√™s ferramentas:

`multiply`, `add` e `divide`.

Ligamos essas ferramentas ao modelo, passamos uma mensagem de sistema informando que ele √© um assistente de aritm√©tica. Tudo igual.

```python
from langchain_openai import ChatOpenAI

def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

# This will be a tool
def add(a: int, b: int) -> int:
    """Adds a and b.

    Args:
        a: first int
        b: second int
    """
    return a + b

def divide(a: int, b: int) -> float:
    """Divide a and b.

    Args:
        a: first int
        b: second int
    """
    return a / b

tools = [add, multiply, divide]
llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools(tools)

from langgraph.graph import MessagesState
from langchain_core.messages import HumanMessage, SystemMessage

# System message
sys_msg = SystemMessage(content="You are a helpful assistant tasked with performing arithmetic on a set of inputs.")

# Node
def assistant(state: MessagesState):
   return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}
   
   
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition, ToolNode
from IPython.display import Image, display

# Graph
builder = StateGraph(MessagesState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "assistant")
react_graph = builder.compile()

# Show
display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))
```

![image.png](attachment:8b21af3b-4ede-4345-aed2-8c3119bbf807:image.png)

### üß© Arquitetura

A estrutura do grafo √©:

- O n√≥ `assistant` recebe a entrada do usu√°rio.
- Se for uma chamada de ferramenta, vai para o n√≥ de ferramentas.
- Depois, volta ao `assistant`, e repete enquanto houver chamadas de ferramentas.
- Quando n√£o houver mais, o grafo termina.

---

### ‚ñ∂Ô∏è Executando sem mem√≥ria

Rodamos a entrada: `"some 3 e 4"` ‚Üí resultado `7`.

Agora, em **uma nova execu√ß√£o**, passamos `"multiplique isso por 2"`.

Problema:

O agente **n√£o sabe o que √© "isso"**.

Ele entende como `2 * 2`, e n√£o `7 * 2`.

Por qu√™? Porque **o estado n√£o √© persistido** entre execu√ß√µes do grafo.

Cada execu√ß√£o come√ßa com um estado novo e **n√£o compartilha mem√≥ria com execu√ß√µes anteriores**.

---

### üíæ Solu√ß√£o: usando **mem√≥ria com checkpointer**

O LangGraph usa **checkpointers** para salvar o estado do grafo ap√≥s cada etapa.

Isso nos d√° **mem√≥ria persistente** entre execu√ß√µes.

A forma mais simples de fazer isso √© usando o `MemorySaver`, um armazenamento de chave-valor em mem√≥ria.

N√≥s n√£o mantemos mem√≥ria do nosso chat inicial!

Isso ocorre porque [o estado √© transit√≥rio](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) durante uma √∫nica execu√ß√£o do grafo.

Claro, isso limita nossa capacidade de ter conversas com m√∫ltiplas intera√ß√µes e interrup√ß√µes.

Podemos usar [persist√™ncia](https://langchain-ai.github.io/langgraph/how-tos/persistence/) para resolver isso!

LangGraph pode usar um checkpointer para salvar automaticamente o estado do grafo ap√≥s cada etapa.

Essa camada de persist√™ncia integrada nos d√° mem√≥ria, permitindo que LangGraph retome a partir da √∫ltima atualiza√ß√£o de estado.

Um dos checkpointers mais f√°ceis de usar √© o `MemorySaver`, um armazenamento chave-valor em mem√≥ria para o estado do Grafo.

Tudo o que precisamos fazer √© compilar o grafo com um checkpointer, e nosso grafo ter√° mem√≥ria!

Tudo o que voc√™ precisa fazer √©:

1. Importar o `MemorySaver`;
2. Compilar o grafo com `checkpointer=MemorySaver()`.

```python
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()
react_graph_memory = builder.compile(checkpointer=memory)
```

Quando usamos mem√≥ria, precisamos especificar um `thread_id`.

Esse `thread_id` armazenar√° nossa cole√ß√£o de estados do grafo.

Aqui est√° uma ilustra√ß√£o:

- O checkpointer grava o estado em cada etapa do grafo
- Esses checkpoints s√£o salvos em uma thread
- Podemos acessar essa thread no futuro usando o `thread_id`

![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e0e9f526b41a4ed9e2d28b_agent-memory2.png)

---

### üìö Como funciona o checkpointer?

Imagine um grafo com dois n√≥s.

Cada etapa gera um **checkpoint** contendo:

- O estado atual,
- O pr√≥ximo n√≥ a ser executado,
- Metadados,
- E um ID de checkpoint.

Esses checkpoints s√£o **agrupados em uma thread** ‚Äî um encadeamento que representa uma sequ√™ncia completa de execu√ß√£o.

Quando voc√™ invoca seu grafo passando um `thread_id`, o LangGraph carrega todos os checkpoints dessa thread e continua a partir do √∫ltimo estado.

```python
# Specify a thread
config = {"configurable": {"thread_id": "1"}}

# Specify an input
messages = [HumanMessage(content="Add 3 and 4.")]

# Run
messages = react_graph_memory.invoke({"messages": messages},config)
for m in messages['messages']:
    m.pretty_print()
```

Se passarmos o mesmo `thread_id`, podemos continuar a partir do checkpoint de estado registrado anteriormente!

Nesse caso, a conversa acima est√° capturada na thread.

A `HumanMessage` que passamos (`"Multiplique isso por 2."`) √© anexada √† conversa acima.

Portanto, o modelo agora sabe que `isso` se refere a `A soma de 3 e 4 √© 7.`.

```python
messages = [HumanMessage(content="Multiply that by 2.")]
messages = react_graph_memory.invoke({"messages": messages}, config)
for m in messages['messages']:
    m.pretty_print()
```

### ‚ñ∂Ô∏è Executando com mem√≥ria

1. Rodamos novamente: `"some 3 e 4"` ‚Üí resultado `7`.
2. Guardamos o `thread_id` gerado.
3. Executamos: `"multiplique isso por 2"` passando o mesmo `thread_id`.

Agora, o agente **sabe** que ‚Äúisso‚Äù √© `7`.

Executa `7 * 2` ‚Üí resultado: **14**.

A mem√≥ria permite **preservar o contexto de execu√ß√µes anteriores**, e o grafo se comporta como se fosse uma conversa cont√≠nua.

---

### üß™ Interagindo com isso no LangGraph Studio

Abrimos o projeto no Studio:

- Abrimos `agent.py`, o mesmo c√≥digo que usamos no notebook.
- Uma observa√ß√£o: **no Studio, n√£o √© necess√°rio definir o checkpointer manualmente**.
    
    O LangGraph API j√° faz isso automaticamente, usando um banco Postgres por tr√°s.
    

No `langgraph.json`, vemos o agente definido como padr√£o.

---

### üßæ Testando no Studio

1. Abrimos o projeto no Studio;
2. Vamos para a aba do agente;
3. Criamos uma nova thread;
4. Entrada: `"multiplique dois por tr√™s"`.

O que acontece:

- O modelo interpreta o input em linguagem natural;
- Transforma em uma chamada de ferramenta estruturada:
    - `multiply(2, 3)`
- O n√≥ de ferramenta executa a fun√ß√£o e retorna `6`;
- O resultado `6` √© passado de volta ao modelo;
- O modelo responde: **‚ÄúO resultado de multiplicar dois por tr√™s √© seis.‚Äù**

---

### ‚úÖ Conclus√£o

Este exemplo mostra como:

- Uma modifica√ß√£o simples com checkpointer pode adicionar **mem√≥ria persistente** ao seu agente;
- Voc√™ pode **encadear execu√ß√µes**, preservando o estado e o hist√≥rico da conversa;
- O LangGraph Studio e a LangGraph API **cuidam da persist√™ncia automaticamente**, facilitando o desenvolvimento.

Esse conceito de mem√≥ria ser√° **amplamente usado** nas pr√≥ximas se√ß√µes e √© uma pe√ßa fundamental para criar agentes conversacionais inteligentes e contextuais.

Se quiser, posso seguir com a pr√≥xima parte!