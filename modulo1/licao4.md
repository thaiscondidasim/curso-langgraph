Anteriormente, constru√≠mos um grafo simples com n√≥s, arestas normais e arestas condicionais.

Agora, vamos avan√ßar para **chains**, que v√£o combinar alguns conceitos fundamentais:

- mensagens de chat,
- modelos de chat,
- liga√ß√£o (binding) de ferramentas,
- e execu√ß√£o de chamadas de ferramentas ‚Äî tudo isso dentro do **LangGraph**.

## Objetivos

Agora, vamos construir uma cadeia simples que combina 4 [conceitos](https://python.langchain.com/v0.2/docs/concepts/):

- Usar [mensagens de chat](https://python.langchain.com/v0.2/docs/concepts/#messages) como nosso estado de gr√°fico
- Usar [modelos de chat](https://python.langchain.com/v0.2/docs/concepts/#chat-models) em n√≥s do gr√°fico
- [Vincular ferramentas](https://python.langchain.com/v0.2/docs/concepts/#tools) ao nosso modelo de chat
- [Executar chamadas de ferramentas](https://python.langchain.com/v0.2/docs/concepts/#functiontool-calling) em n√≥s do gr√°fico

![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)

---

### üìå Primeiros conceitos isolados

### **Mensagens**

Modelos de chat interagem com mensagens.

Aqui est√° um exemplo simples: podemos criar uma lista de mensagens que representam uma conversa entre uma IA e um humano.

Cada mensagem pode ter um **nome** e um **conte√∫do**.

Modelos de chat podem usar [`mensagens`](https://python.langchain.com/v0.2/docs/concepts/#messages), que capturam diferentes pap√©is dentro de uma conversa.

O LangChain suporta v√°rios tipos de mensagens, incluindo:

- `HumanMessage` (Mensagem Humana)
- `AIMessage` (Mensagem de IA)
- `SystemMessage` (Mensagem de Sistema)
- `ToolMessage` (Mensagem de Ferramenta)

Estas representam, respectivamente:

- Uma mensagem do usu√°rio
- Uma mensagem do modelo de chat
- Uma instru√ß√£o para o modelo definir seu comportamento
- Uma mensagem de retorno de uma chamada de ferramenta

Vamos criar uma lista de mensagens.

Cada mensagem pode ser configurada com:

- `content` - o conte√∫do da mensagem
- `name` - opcionalmente, o autor da mensagem
- `response_metadata` - opcionalmente, um dicion√°rio de metadados (geralmente preenchido pelo provedor do modelo para `AIMessages`)

Ao imprimir essa lista, temos algo assim:

```python
from pprint import pprint
from langchain_core.messages import AIMessage, HumanMessage

messages = [AIMessage(content=f"So you said you were researching ocean mammals?", name="Model")]
messages.append(HumanMessage(content=f"Yes, that's right.",name="Lance"))
messages.append(AIMessage(content=f"Great, what would you like to learn about.", name="Model"))
messages.append(HumanMessage(content=f"I want to learn about the best place to see Orcas in the US.", name="Lance"))

for m in messages:
    m.pretty_print()
```

![image.png](attachment:0177e1cb-fc61-4de0-b305-be5df67f46c4:image.png)

Agora podemos passar essa lista diretamente para um modelo de chat.

Primeiro, garantimos que a chave da OpenAI est√° definida:

```python
import os, getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

Importamos `ChatOpenAI`, especificamos o modelo (neste caso `gpt-4-0`) e invocamos o modelo com a lista de mensagens.

```python
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o")
result = llm.invoke(messages)
type(result)
```

O resultado ser√° uma mensagem da IA.

```python
AIMessage(content='One of the best places to see orcas in the United States is the Pacific Northwest, particularly around the San Juan Islands in Washington State. Here are some details:\n\n1. **San Juan Islands, Washington**: These islands are a renowned spot for whale watching, with orcas frequently spotted between late spring and early fall. The waters around the San Juan Islands are home to both resident and transient orca pods, making it an excellent location for sightings.\n\n2. **Puget Sound, Washington**: This area, including places like Seattle and the surrounding waters, offers additional opportunities to see orcas, particularly the Southern Resident killer whale population.\n\n3. **Olympic National Park, Washington**: The coastal areas of the park provide a stunning backdrop for spotting orcas, especially during their migration periods.\n\nWhen planning a trip for whale watching, consider peak seasons for orca activity and book tours with reputable operators who adhere to responsible wildlife viewing practices. Additionally, land-based spots like Lime Kiln Point State Park, also known as ‚ÄúWhale Watch Park,‚Äù on San Juan Island, offer great opportunities for orca watching from shore.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 228, 'prompt_tokens': 67, 'total_tokens': 295, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-57ed2891-c426-4452-b44b-15d0a5c3f225-0', usage_metadata={'input_tokens': 67, 'output_tokens': 228, 'total_tokens': 295, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})

```

O conte√∫do ser√° uma string com a resposta do LLM, e tamb√©m teremos metadados da resposta, como informa√ß√µes sobre os tokens usados no prompt, nome do modelo, etc.

---

### üõ†Ô∏è Ferramentas (Tools)

Agora, vamos introduzir a ideia de **ferramentas** ‚Äî outra forma de usar modelos de chat.

A ideia √© simples: √†s vezes queremos que o modelo se conecte com uma **ferramenta externa**, como uma API que requer um payload espec√≠fico.

Quando vinculamos uma API como ferramenta, damos ao modelo conhecimento sobre o esquema de entrada necess√°rio.

O modelo decidir√° chamar uma ferramenta com base na entrada em linguagem natural do usu√°rio.

E retornar√° uma sa√≠da que adere ao esquema da ferramenta.

[Muitos provedores de LLM suportam chamadas de ferramentas](https://python.langchain.com/v0.1/docs/integrations/chat/) e a [interface de chamada de ferramentas](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/) no langchain √© f√°cil de fazer.

Voc√™ pode simplesmente passar qualquer fun√ß√£o Python para `ChatModel.bind_tools(fun√ß√£o)`.

![](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dc1c17a7a57f9960_chain2.png)

Exemplo:

Criamos uma fun√ß√£o chamada `multiply` que recebe `a` e `b`.

```python
def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

llm_with_tools = llm.bind_tools([multiply])
```

Usamos `llm.bind_tools` para associar essa fun√ß√£o ao modelo.

Agora o modelo est√° **ciente** dessa fun√ß√£o.

Como no diagrama:

- Entramos com uma linguagem natural,
- E o modelo gera o payload necess√°rio para executar a fun√ß√£o.

Vamos testar:

Invocamos o modelo com a pergunta ‚ÄúQual √© 2 multiplicado por 3?‚Äù.

```python
tool_call = llm_with_tools.invoke([HumanMessage(content=f"What is 2 multiplied by 3", name="Lance")])
```

O resultado: a mensagem da IA n√£o tem um conte√∫do direto, mas sim uma **chamada de ferramenta**.

```python
tool_call.tool_calls
[{'name': 'multiply',
  'args': {'a': 2, 'b': 3},
  'id': 'call_lBBBNo5oYpHGRqwxNaNRbsiT',
  'type': 'tool_call'}]
```

Ela inclui os argumentos e o nome da fun√ß√£o ‚Äî bem legal.

---

## Usando mensagens como estado

Com essas bases estabelecidas, podemos agora usar [`mensagens`](https://python.langchain.com/v0.2/docs/concepts/#messages) como estado em nosso grafo.

Vamos definir nosso estado, `MessagesState`, como um `TypedDict` com uma √∫nica chave: `messages`.

`messages` √© simplesmente uma lista de mensagens, como definimos anteriormente (por exemplo, `HumanMessage`, etc.).

```python
from typing_extensions import TypedDict
from langchain_core.messages import AnyMessage

class MessagesState(TypedDict):
    messages: list[AnyMessage]
```

---

## Reducers

Agora, temos um pequeno problema!

Como discutimos, cada n√≥ retornar√° um novo valor para nossa chave de estado `messages`.

Por√©m, esse novo valor [substituir√°](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) o valor anterior de `messages`.

√Ä medida que nosso grafo √© executado, queremos **acrescentar** mensagens √† nossa chave de estado `messages`, n√£o substitu√≠-las.

Podemos resolver isso usando [fun√ß√µes redutoras](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers).

### Como os redutores funcionam:

1. **Comportamento padr√£o**:
    - Sem redutor especificado: atualiza√ß√µes substituem o valor anterior
    - Exemplo: `messages = novas_mensagens` (sobrescreve)
2. **Redutor `add_messages`**:
    - Especifica que queremos concatenar as listas
    - Exemplo: `messages = messages_anteriores + novas_mensagens`

### Implementa√ß√£o:

Basta anotar nossa chave `messages` com a fun√ß√£o redutora `add_messages` como metadado:

```python
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages

# Definindo o esquema de estado com redutor
class MessagesState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]  # Anota√ß√£o especial

# Criando o grafo
workflow = StateGraph(MessagesState)

```

Isso garante que todas as novas mensagens sejam automaticamente anexadas √† lista existente durante a execu√ß√£o do grafo.

Como ter uma lista de mensagens em estado de grafo √© muito comum, o LangGraph possui um [`MessagesState`](https://langchain-ai.github.io/langgraph/concepts/low_level/#messagesstate) pr√©-constru√≠do!

O `MessagesState` √© definido:

- Com uma √∫nica chave `messages` pr√©-configurada
- Esta √© uma lista de objetos `AnyMessage`
- Ele usa o redutor `add_messages`

Geralmente usaremos o `MessagesState` porque √© menos verboso do que definir um `TypedDict` personalizado, como mostrado acima.

```python
from langgraph.graph import MessagesState

class MessagesState(MessagesState):
    # Add any keys needed beyond messages, which is pre-built 
    pass
```

Para aprofundar um pouco mais, podemos ver como o redutor `add_messages` funciona de forma isolada.  

```python
# Initial state
initial_messages = [AIMessage(content="Hello! How can I assist you?", name="Model"),
                    HumanMessage(content="I'm looking for information on marine biology.", name="Lance")
                   ]

# New message to add
new_message = AIMessage(content="Sure, I can help with that. What specifically are you interested in?", name="Model")

# Test
add_messages(initial_messages , new_message)
```

Output:

```python
================================== Ai Message ==================================
Name: Model

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, that's right.
================================== Ai Message ==================================
Name: Model

Great, what would you like to learn about.
================================ Human Message =================================
Name: Lance

I want to learn about the best place to see Orcas in the US.
langchain_core.messages.ai.AIMessage
AIMessage(content='One of the best places to see orcas in the United States is the Pacific Northwest, particularly around the San Juan Islands in Washington State. Here are some details:\n\n1. **San Juan Islands, Washington**: These islands are a renowned spot for whale watching, with orcas frequently spotted between late spring and early fall. The waters around the San Juan Islands are home to both resident and transient orca pods, making it an excellent location for sightings.\n\n2. **Puget Sound, Washington**: This area, including places like Seattle and the surrounding waters, offers additional opportunities to see orcas, particularly the Southern Resident killer whale population.\n\n3. **Olympic National Park, Washington**: The coastal areas of the park provide a stunning backdrop for spotting orcas, especially during their migration periods.\n\nWhen planning a trip for whale watching, consider peak seasons for orca activity and book tours with reputable operators who adhere to responsible wildlife viewing practices. Additionally, land-based spots like Lime Kiln Point State Park, also known as ‚ÄúWhale Watch Park,‚Äù on San Juan Island, offer great opportunities for orca watching from shore.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 228, 'prompt_tokens': 67, 'total_tokens': 295, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-57ed2891-c426-4452-b44b-15d0a5c3f225-0', usage_metadata={'input_tokens': 67, 'output_tokens': 228, 'total_tokens': 295, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
{'token_usage': {'completion_tokens': 228,
  'prompt_tokens': 67,
  'total_tokens': 295,
  'completion_tokens_details': {'accepted_prediction_tokens': 0,
   'audio_tokens': 0,
   'reasoning_tokens': 0,
   'rejected_prediction_tokens': 0},
  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},
 'model_name': 'gpt-4o-2024-08-06',
 'system_fingerprint': 'fp_50cad350e4',
 'finish_reason': 'stop',
 'logprobs': None}
[{'name': 'multiply',
  'args': {'a': 2, 'b': 3},
  'id': 'call_lBBBNo5oYpHGRqwxNaNRbsiT',
  'type': 'tool_call'}]
[AIMessage(content='Hello! How can I assist you?', name='Model', id='cd566566-0f42-46a4-b374-fe4d4770ffa7'),
 HumanMessage(content="I'm looking for information on marine biology.", name='Lance', id='9b6c4ddb-9de3-4089-8d22-077f53e7e915'),
 AIMessage(content='Sure, I can help with that. What specifically are you interested in?', name='Model', id='74a549aa-8b8b-48d4-bdf1-12e98404e44e')]
<IPython.core.display.Image object>
================================ Human Message =================================

Hello!
================================== Ai Message ==================================

Hi there! How can I assist you today?
================================ Human Message =================================

Multiply 2 and 3!
================================== Ai Message ==================================
Tool Calls:
  multiply (call_Er4gChFoSGzU7lsuaGzfSGTQ)
 Call ID: call_Er4gChFoSGzU7lsuaGzfSGTQ
  Args:
    a: 2
    b: 3
```

---

## Nosso grafo

Agora, vamos usar o `MessagesState` com um grafo.

```python
from IPython.display import Image, display
from langgraph.graph import StateGraph, START, END
    
# Node
def tool_calling_llm(state: MessagesState):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# Build graph
builder = StateGraph(MessagesState)
builder.add_node("tool_calling_llm", tool_calling_llm)
builder.add_edge(START, "tool_calling_llm")
builder.add_edge("tool_calling_llm", END)
graph = builder.compile()

# View
display(Image(graph.get_graph().draw_mermaid_png()))

```

![image.png](attachment:61621ec1-bcc0-4eb5-9c87-e3725071e8fa:image.png)

---

Se passarmos `"Ol√°!"`, o LLM responde sem nenhuma chamada de ferramenta.  

```python
messages = graph.invoke({"messages": HumanMessage(content="Hello!")})
for m in messages['messages']:
    m.pretty_print()
```

Output:

```python
================================== Ai Message ==================================
Name: Model

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, that's right.
================================== Ai Message ==================================
Name: Model

Great, what would you like to learn about.
================================ Human Message =================================
Name: Lance

I want to learn about the best place to see Orcas in the US.
langchain_core.messages.ai.AIMessage
AIMessage(content='One of the best places to see orcas in the United States is the Pacific Northwest, particularly around the San Juan Islands in Washington State. Here are some details:\n\n1. **San Juan Islands, Washington**: These islands are a renowned spot for whale watching, with orcas frequently spotted between late spring and early fall. The waters around the San Juan Islands are home to both resident and transient orca pods, making it an excellent location for sightings.\n\n2. **Puget Sound, Washington**: This area, including places like Seattle and the surrounding waters, offers additional opportunities to see orcas, particularly the Southern Resident killer whale population.\n\n3. **Olympic National Park, Washington**: The coastal areas of the park provide a stunning backdrop for spotting orcas, especially during their migration periods.\n\nWhen planning a trip for whale watching, consider peak seasons for orca activity and book tours with reputable operators who adhere to responsible wildlife viewing practices. Additionally, land-based spots like Lime Kiln Point State Park, also known as ‚ÄúWhale Watch Park,‚Äù on San Juan Island, offer great opportunities for orca watching from shore.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 228, 'prompt_tokens': 67, 'total_tokens': 295, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-57ed2891-c426-4452-b44b-15d0a5c3f225-0', usage_metadata={'input_tokens': 67, 'output_tokens': 228, 'total_tokens': 295, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
{'token_usage': {'completion_tokens': 228,
  'prompt_tokens': 67,
  'total_tokens': 295,
  'completion_tokens_details': {'accepted_prediction_tokens': 0,
   'audio_tokens': 0,
   'reasoning_tokens': 0,
   'rejected_prediction_tokens': 0},
  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},
 'model_name': 'gpt-4o-2024-08-06',
 'system_fingerprint': 'fp_50cad350e4',
 'finish_reason': 'stop',
 'logprobs': None}
[{'name': 'multiply',
  'args': {'a': 2, 'b': 3},
  'id': 'call_lBBBNo5oYpHGRqwxNaNRbsiT',
  'type': 'tool_call'}]
[AIMessage(content='Hello! How can I assist you?', name='Model', id='cd566566-0f42-46a4-b374-fe4d4770ffa7'),
 HumanMessage(content="I'm looking for information on marine biology.", name='Lance', id='9b6c4ddb-9de3-4089-8d22-077f53e7e915'),
 AIMessage(content='Sure, I can help with that. What specifically are you interested in?', name='Model', id='74a549aa-8b8b-48d4-bdf1-12e98404e44e')]
<IPython.core.display.Image object>
================================ Human Message =================================

Hello!
================================== Ai Message ==================================

Hi there! How can I assist you today?
================================ Human Message =================================

Multiply 2 and 3!
================================== Ai Message ==================================
Tool Calls:
  multiply (call_Er4gChFoSGzU7lsuaGzfSGTQ)
 Call ID: call_Er4gChFoSGzU7lsuaGzfSGTQ
  Args:
    a: 2
    b: 3
```

O LLM decide usar uma ferramenta quando determina que a entrada ou tarefa requer a funcionalidade fornecida por essa ferramenta.

```python
messages = graph.invoke({"messages": HumanMessage(content="Multiply 2 and 3")})
for m in messages['messages']:
    m.pretty_print()
```

Output:

```python
================================== Ai Message ==================================
Name: Model

So you said you were researching ocean mammals?
================================ Human Message =================================
Name: Lance

Yes, that's right.
================================== Ai Message ==================================
Name: Model

Great, what would you like to learn about.
================================ Human Message =================================
Name: Lance

I want to learn about the best place to see Orcas in the US.
langchain_core.messages.ai.AIMessage
AIMessage(content='One of the best places to see orcas in the United States is the Pacific Northwest, particularly around the San Juan Islands in Washington State. Here are some details:\n\n1. **San Juan Islands, Washington**: These islands are a renowned spot for whale watching, with orcas frequently spotted between late spring and early fall. The waters around the San Juan Islands are home to both resident and transient orca pods, making it an excellent location for sightings.\n\n2. **Puget Sound, Washington**: This area, including places like Seattle and the surrounding waters, offers additional opportunities to see orcas, particularly the Southern Resident killer whale population.\n\n3. **Olympic National Park, Washington**: The coastal areas of the park provide a stunning backdrop for spotting orcas, especially during their migration periods.\n\nWhen planning a trip for whale watching, consider peak seasons for orca activity and book tours with reputable operators who adhere to responsible wildlife viewing practices. Additionally, land-based spots like Lime Kiln Point State Park, also known as ‚ÄúWhale Watch Park,‚Äù on San Juan Island, offer great opportunities for orca watching from shore.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 228, 'prompt_tokens': 67, 'total_tokens': 295, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-57ed2891-c426-4452-b44b-15d0a5c3f225-0', usage_metadata={'input_tokens': 67, 'output_tokens': 228, 'total_tokens': 295, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
{'token_usage': {'completion_tokens': 228,
  'prompt_tokens': 67,
  'total_tokens': 295,
  'completion_tokens_details': {'accepted_prediction_tokens': 0,
   'audio_tokens': 0,
   'reasoning_tokens': 0,
   'rejected_prediction_tokens': 0},
  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},
 'model_name': 'gpt-4o-2024-08-06',
 'system_fingerprint': 'fp_50cad350e4',
 'finish_reason': 'stop',
 'logprobs': None}
[{'name': 'multiply',
  'args': {'a': 2, 'b': 3},
  'id': 'call_lBBBNo5oYpHGRqwxNaNRbsiT',
  'type': 'tool_call'}]
[AIMessage(content='Hello! How can I assist you?', name='Model', id='cd566566-0f42-46a4-b374-fe4d4770ffa7'),
 HumanMessage(content="I'm looking for information on marine biology.", name='Lance', id='9b6c4ddb-9de3-4089-8d22-077f53e7e915'),
 AIMessage(content='Sure, I can help with that. What specifically are you interested in?', name='Model', id='74a549aa-8b8b-48d4-bdf1-12e98404e44e')]
<IPython.core.display.Image object>
================================ Human Message =================================

Hello!
================================== Ai Message ==================================

Hi there! How can I assist you today?
================================ Human Message =================================

Multiply 2 and 3!
================================== Ai Message ==================================
Tool Calls:
  multiply (call_Er4gChFoSGzU7lsuaGzfSGTQ)
 Call ID: call_Er4gChFoSGzU7lsuaGzfSGTQ
  Args:
    a: 2
    b: 3
```

### üîÅ Integrando tudo no LangGraph

Vamos agora integrar tudo isso ao LangGraph.

Primeiro, como usar mensagens como **estado do grafo**?

Simples: definimos uma classe `MessagesState`, que √© um `TypedDict` com uma chave `messages`, que √© uma lista de mensagens.

Por√©m, h√° um detalhe:

No LangGraph, por padr√£o, quando fazemos atualiza√ß√µes de estado, o valor da chave √© sobrescrito.

Mas nesse caso, **n√£o queremos sobrescrever**, e sim **acrescentar** cada nova mensagem ‚Äî para manter o hist√≥rico da conversa.

Para isso, usamos **fun√ß√µes redutoras** (reducer functions).

Podemos anotar a chave `messages` com uma fun√ß√£o redutora que instrui o LangGraph a **acrescentar** novas mensagens em vez de sobrescrever.

Isso √© t√£o comum que j√° existe uma fun√ß√£o redutora pronta chamada `add_messages_reducer`, e inclusive um estado chamado `MessagesState` com essa fun√ß√£o embutida.

√â s√≥ usar.

### Exemplo isolado:

Temos uma lista de mensagens.

Queremos adicionar uma nova mensagem.

Rodamos `add_messages_reducer`, e ela √© adicionada corretamente.

Legal, agora sabemos como funciona.

---

### üîó Criando o grafo

Vamos criar um grafo simples:

- Definimos o `MessagesState`;
- Criamos um √∫nico n√≥: o modelo de chat com ferramenta ligada;
- Esse n√≥ recebe as mensagens do estado e executa;
- Adicionamos as arestas: de `start` ‚Üí `llm com ferramenta` ‚Üí `end`.

Compilamos e visualizamos o grafo.

Perfeito. Come√ßa, passa pelo modelo com ferramenta, termina.

---

### üß™ Testando o grafo

Agora vamos testar o grafo com dois tipos de entrada:

1. **Entrada simples**: `"hello"`
    
    Executamos o grafo.
    
    A mensagem da IA √©: **"Ol√°, como posso ajudar?"** ‚Äî exatamente como esperado.
    
2. **Entrada com ferramenta**: `"Qual √© 2 vezes 3?"`
    
    Executamos novamente.
    
    A IA **n√£o** responde diretamente, mas retorna uma **chamada de ferramenta**.
    
    Vemos os argumentos e o nome da fun√ß√£o `multiply`.
    

---

Tudo funciona como vimos nos testes isolados, mas agora integrado como um grafo no **LangGraph**.

